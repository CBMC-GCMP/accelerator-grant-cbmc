[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "We are the Centro para la Biodiversidad Marina y la Conservación, A.C., based in Mexico. We had the luck to be supported by the Patrick J. McGovern foundation accelerator grant, which allowed us to grow our data maturity as an organization."
  },
  {
    "objectID": "code/modelo-PGeneral.html",
    "href": "code/modelo-PGeneral.html",
    "title": "Final Insight Report CBMC",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport os\nfrom pandas import read_csv\nimport tensorflow\nfrom keras.models import Sequential\nfrom keras.layers.core import Dropout\nfrom keras.layers import  Dropout,ConvLSTM2D,Conv3DTranspose,Dense,ReLU\nimport imageio\nfrom functools import reduce\nimport matplotlib.pyplot as plt\nimport cv2\nimport tqdm\nimport time\nfrom datetime import datetime as dt\nimport tensorflow as tf\nimport scipy.stats\nfrom scipy import stats\n\n/usr/local/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.2\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\""
  },
  {
    "objectID": "code/modelo-PGeneral.html#pre-procesing",
    "href": "code/modelo-PGeneral.html#pre-procesing",
    "title": "Final Insight Report CBMC",
    "section": "Pre-procesing",
    "text": "Pre-procesing\n\nPrepare SST\n\n#conect to the bucket\nbucket3 = 's3://acc-cmbc-vms/my-data/climate_scenarios/GFDL85/monthly_data' \ndata_key3 = '_tos_gfdl_ssp585_1850_2014_monthly.csv' \ndata_location3 = 's3://{}/{}'.format(bucket3,data_key3) \nsst0 = pd.read_csv(data_location3)\ndata_key4 = '_tos_gfdl_ssp585_2015_2100_monthly.csv' \ndata_location4 = 's3://{}/{}'.format(bucket3,data_key4)\nsst2 = pd.read_csv(data_location4)\nsst = pd.concat([sst0, sst2], ignore_index = True, axis = 0)\nsst = sst.set_index('date')\nsst = sst.loc['2008-01-01':'2021-01-01']#extract only the data for the training\nsst = sst.reset_index()\ndate = sst['date']\ndate_list1 = list(sst['date'])\ntt = pd.DataFrame(date_list1)\ndate_list1 = tt[0].unique()\nsst['date'] = pd.to_datetime(sst['date'])\nsst = sst.set_index(['date'])\nsst = sst.drop(columns=['esm','ssp'])\ndel sst0,sst2\n\n\ndef transo1(data,date_list):\n    x1 = data.loc[date_list]\n    x1 = x1.reset_index()\n    op = data.columns\n    table = pd.pivot_table(x1, values=op[0], index=['lat'],columns=['lon']).fillna(0)\n    x2 = np.flipud(cv2.resize(np.array(table),[65,16]))\n    dt = pd.DataFrame(x2, columns=col, index=ind)\n    x = pd.DataFrame(unpivot(dt))\n    fecha = np.repeat(date_list,len(x))\n    x['date'] = fecha\n    return x\n\n\ndef vectoriza(data,date_list):\n    for i in tqdm.tqdm(range(0,len(date_list))):\n        if i==0:\n            x = transo1(data,date_list[i])\n        else:\n            x1 = transo1(data,date_list[i])\n            x = pd.concat([x,x1], ignore_index = True, axis = 0)\n    return x\n\n\ntable2 = pd.pivot_table(sst.loc['2008-01-01'], values='tos', index=['lat'],columns=['lon'])\ncol = np.array(table2.columns)\nind = np.array(table2.index)\n\n\nsstv = vectoriza(sst,date_list1)\n\n100%|██████████| 157/157 [00:01<00:00, 84.67it/s]\n\n\n\nsstv.rename(columns = {'value':'tos'}, inplace = True)\n\n\n\nO2\n\ndata_key4 = '_o2_gfdl_ssp585_1850_2014_monthly.csv' #key or the name of the file\ndata_location4 = 's3://{}/{}'.format(bucket3,data_key4) #create the complete path in the format\nO2_surf1 = pd.read_csv(data_location4)\ndata_key5 = '_o2_gfdl_ssp585_2015_2100_monthly.csv' #key or the name of the file\ndata_location4 = 's3://{}/{}'.format(bucket3,data_key5) #create the complete path in the format\nO2_surf2 = pd.read_csv(data_location4)\nO2 = pd.concat([O2_surf1, O2_surf2], ignore_index= True, axis=0)\nO2 = O2.set_index('date')\nO2 = O2.loc['2008-01-01':'2021-01-01']\nO2 = O2.reset_index()\nO2['date'] = pd.to_datetime(O2['date'])\nO2 = O2.set_index(['date'])\nO2 = O2.drop(columns=['esm','ssp'])\ndel O2_surf1, O2_surf2\n\n\no2v = vectoriza(O2,date_list1)\n\n100%|██████████| 157/157 [00:01<00:00, 84.59it/s]\n\n\n\no2v.rename(columns = {'value':'o2'}, inplace = True)\n\n\n\nintpp\n\nbucket4 = 's3://acc-cmbc-vms/my-data/climate_scenarios/GFDL85/monthly_data' #this is the paath to the data\ndata_key4 = '_intpp_gfdl_ssp585_1850_2014_monthly.csv' #key or the name of the file\ndata_location4 = 's3://{}/{}'.format(bucket3,data_key4) #create the complete path in the format\nintpp1 = pd.read_csv(data_location4)\ndata_key5 = '_intpp_gfdl_ssp585_2015_2100_monthly.csv' #key or the name of the file\ndata_location4 = 's3://{}/{}'.format(bucket3,data_key5) #create the complete path in the format\nintpp2 = pd.read_csv(data_location4)\nintpp = pd.concat([intpp1, intpp2], ignore_index= True, axis=0)\nintpp = intpp.set_index('date')\nintpp = intpp.loc['2008-01-01':'2021-01-01']\nintpp = intpp.reset_index()\nintpp['date'] = pd.to_datetime(intpp['date'])\nintpp = intpp.set_index(['date'])\nintpp = intpp.drop(columns=['esm','ssp'])\ndel intpp1, intpp2\n\n\nintppv  = vectoriza(intpp,date_list1)\n\n100%|██████████| 157/157 [00:01<00:00, 85.16it/s]\n\n\n\nintppv.rename(columns = {'value':'intpp'}, inplace = True)\n\n\n\npH\n\nbucket4 = 's3://acc-cmbc-vms/my-data/climate_scenarios/GFDL85/monthly_data' #this is the paath to the data\ndata_key4 = '_ph_gfdl_ssp585_1850_2014_monthly.csv' #key or the name of the file\ndata_location4 = 's3://{}/{}'.format(bucket4,data_key4) #create the complete path in the format\npH1 = pd.read_csv(data_location4)\ndata_key5 = '_ph_gfdl_ssp585_2015_2100_monthly.csv' #key or the name of the file\ndata_location4 = 's3://{}/{}'.format(bucket4,data_key5) #create the complete path in the format\npH2 = pd.read_csv(data_location4)\npH = pd.concat([pH1, pH2], ignore_index= True, axis=0)\npH = pH.set_index('date')\npH = pH.loc['2008-01-01':'2021-01-01']\npH = pH.reset_index()\npH['date'] = pd.to_datetime(pH['date'])\npH = pH.set_index(['date'])\npH = pH.drop(columns=['esm','ssp'])\ndel pH1, pH2\n\n\npHv = vectoriza(pH,date_list1)\n\n100%|██████████| 157/157 [00:01<00:00, 82.36it/s]\n\n\n\npHv.rename(columns = {'value':'pH'}, inplace = True)\n\n\n\nSO\n\nbucket4 = 's3://acc-cmbc-vms/my-data/climate_scenarios/GFDL85/monthly_data' #this is the paath to the data\ndata_key4 = '_so_gfdl_ssp585_1850_2014_monthly.csv' #key or the name of the file\ndata_location4 = 's3://{}/{}'.format(bucket4,data_key4) #create the complete path in the format\nso1 = pd.read_csv(data_location4)\ndata_key5 = '_so_gfdl_ssp585_2015_2100_monthly.csv' #key or the name of the file\ndata_location4 = 's3://{}/{}'.format(bucket4,data_key5) #create the complete path in the format\nso2 = pd.read_csv(data_location4)\nso = pd.concat([so1, so2], ignore_index= True, axis=0)\nso = so.set_index('date')\nso = so.loc['2008-01-01':'2021-01-01']\nso = so.reset_index()\nso['date'] = pd.to_datetime(so['date'])\nso = so.set_index(['date'])\nso = so.drop(columns=['esm','ssp'])\ndel so1, so2\n\n\nsov  = vectoriza(so,date_list1)\n\n100%|██████████| 157/157 [00:01<00:00, 82.20it/s]\n\n\n\nsov.rename(columns = {'value':'so'}, inplace = True)\n\n\n\nLoad VMS points\n\nbucket = 's3://acc-cmbc-vms/my-data/datasets/fisheries_mtx' #this is the paath to the data\ndata_key = 'vms_data_point_one.csv' #key or the name of the file\ndata_location = 's3://{}/{}'.format(bucket,data_key) #create the complete path in the format\ndata = pd.read_csv(data_location)\ndata = data.drop(columns=['Unnamed: 0'])\ndata.rename(columns = {'latitude':'lat', 'longitude':'lon'}, inplace = True)\ndate_list = list(data['date'])\ntt = pd.DataFrame(date_list)\ndate_list = tt[0].unique()\ndata = data.set_index(['date'])\n\n\nEscalamiento de los datos de 0.1 a 0.5\n\ndef transo(data,date_list):\n    x1 = data.loc[str(date_list)]\n    x1 = x1.reset_index()\n    table = pd.pivot_table(x1, values='yeld', index=['lat'],columns=['lon']).fillna(0)\n    x2 = cv2.resize(np.array(table),[16,65])\n    dt = pd.DataFrame(x2.transpose(), columns=col, index=ind)\n    x = pd.DataFrame(unpivot(dt))\n    fecha = np.repeat(date_list,len(x))\n    x['date'] = fecha\n    return x\n\n\nfor i in tqdm.tqdm(range(0,len(date_list))):\n    if i==0:\n        x = transo(data,date_list[i])\n    else:\n        x1 = transo(data,date_list[i])\n        x = pd.concat([x,x1], ignore_index = True, axis = 0)\n\n100%|██████████| 428/428 [00:09<00:00, 44.38it/s]\n\n\n\nx.rename(columns = {'value':'yeld'}, inplace = True)"
  },
  {
    "objectID": "code/modelo-PGeneral.html#unión-de-tablas-de-input",
    "href": "code/modelo-PGeneral.html#unión-de-tablas-de-input",
    "title": "Final Insight Report CBMC",
    "section": "Unión de tablas de input",
    "text": "Unión de tablas de input\n\ndatos = pd.DataFrame([sstv['date'],sstv['lat'],sstv['lon'],sstv['tos'],o2v['o2'],intppv['intpp'],pHv['pH'],sov['so']]).transpose()\ndatos['date'] = pd.to_datetime(datos['date'])\ndatos['date'] = datos['date'].dt.strftime('%Y-%m')\n\n\nx['date'] = pd.to_datetime(x['date'])\nx['date'] = x['date'].dt.strftime('%Y-%m')\nDATA = x.merge(datos)\nDATA.to_csv('DATA.csv')\n\n\nDatos para pronóstio\n\nbucket3 = 's3://acc-cmbc-vms/my-data/climate_scenarios/GFDL85/monthly_data' \n\ndata_key4 = '_tos_gfdl_ssp585_2015_2100_monthly.csv' \ndata_location4 = 's3://{}/{}'.format(bucket3,data_key4)\nssti = pd.read_csv(data_location4)\nssti = ssti.set_index('date')\nssti = ssti.loc['2021-01-01':]\nssti = ssti.reset_index()\ndate_list2 = list(ssti['date'])\ntt = pd.DataFrame(date_list2)\ndate_list2 = tt[0].unique()\nssti = ssti.set_index(['date'])\nssti = ssti.drop(columns=['esm','ssp'])\nsstvi = vectoriza(ssti,date_list2)\nsstvi.rename(columns = {'value':'tos'}, inplace = True)\n\ndata_key5 = '_o2_gfdl_ssp585_2015_2100_monthly.csv' #key or the name of the file\ndata_location4 = 's3://{}/{}'.format(bucket3,data_key5) #create the complete path in the format\nO2i = pd.read_csv(data_location4)\nO2i = O2i.set_index('date')\nO2i = O2i.loc['2021-01-01':]\nO2i = O2i.reset_index()\nO2i['date'] = pd.to_datetime(O2i['date'])\nO2i = O2i.set_index(['date'])\nO2i = O2i.drop(columns=['esm','ssp'])\no2vi = vectoriza(O2i,date_list2)\no2vi.rename(columns = {'value':'o2'}, inplace = True)\n\ndata_key5 = '_intpp_gfdl_ssp585_2015_2100_monthly.csv' \ndata_location4 = 's3://{}/{}'.format(bucket3,data_key5)\nintppi = pd.read_csv(data_location4)\nintppi = intppi.set_index('date')\nintppi = intppi.loc['2021-01-01':]\nintppi = intppi.reset_index()\nintppi['date'] = pd.to_datetime(intppi['date'])\nintppi = intppi.set_index(['date'])\nintppi = intppi.drop(columns=['esm','ssp'])\nintppvi  = vectoriza(intppi,date_list2)\nintppvi.rename(columns = {'value':'intpp'}, inplace = True)\n\ndata_key5 = '_ph_gfdl_ssp585_2015_2100_monthly.csv' #key or the name of the file\ndata_location4 = 's3://{}/{}'.format(bucket3,data_key5) #create the complete path in the format\npHi = pd.read_csv(data_location4)\npHi = pHi.set_index('date')\npHi = pHi.loc['2021-01-01':]\npHi = pHi.reset_index()\npHi['date'] = pd.to_datetime(pHi['date'])\npHi = pHi.set_index(['date'])\npHi = pHi.drop(columns=['esm','ssp'])\npHvi = vectoriza(pHi,date_list2)\npHvi.rename(columns = {'value':'pH'}, inplace = True)\n\ndata_key5 = '_so_gfdl_ssp585_2015_2100_monthly.csv' #key or the name of the file\ndata_location4 = 's3://{}/{}'.format(bucket3,data_key5) #create the complete path in the format\nsoi = pd.read_csv(data_location4)\nsoi = soi.set_index('date')\nsoi = soi.loc['2021-01-01':]\nsoi = soi.reset_index()\nsoi['date'] = pd.to_datetime(soi['date'])\nsoi = soi.set_index(['date'])\nsoi= soi.drop(columns=['esm','ssp'])\nsovi  = vectoriza(soi,date_list2)\nsovi.rename(columns = {'value':'so'}, inplace = True)\n\ndatos2 = pd.DataFrame([sstvi['date'],sstvi['lat'],sstvi['lon'],sstvi['tos'],o2vi['o2'],intppvi['intpp'],pHvi['pH'],sovi['so']]).transpose()\ndatos2['date'] = pd.to_datetime(datos2['date'])\ndatos2['date'] = datos2['date'].dt.strftime('%Y-%m')\ndatos2.to_csv('input_fore.csv')\n\n100%|██████████| 960/960 [00:19<00:00, 50.45it/s]\n100%|██████████| 960/960 [00:19<00:00, 49.44it/s]\n100%|██████████| 960/960 [00:20<00:00, 47.19it/s]\n100%|██████████| 960/960 [00:20<00:00, 47.86it/s]\n100%|██████████| 960/960 [00:19<00:00, 50.11it/s]\n\n\n\n\nEstandarización de los datos\n\ndef probabilidad(X):\n    X=stats.zscore(X)\n    P=scipy.stats.norm.sf(abs(X))\n    return P\n\n\ndef mimax(X):\n    Max = X.max()\n    Min = X.min()\n    Delta = Max - Min\n    STD = (X-Min)/Delta \n    return STD,Max,Min\n\n\ndef demimax(X,Max,Min):\n    DST = X*(Max-Min)+Min\n    return DST\n\n\nSTD_yeld = probabilidad(DATA['yeld'])\nSTD_tos,Max_tos,Min_tos = mimax(DATA['tos'])\nSTD_o2,Max_o2,Min_o2 = mimax(DATA['o2'])\nSTD_intpp,Max_intpp,Min_intpp = mimax(DATA['intpp'])\nSTD_pH,Max_pH,Min_pH = mimax(DATA['pH'])\nSTD_so,Max_so,Min_so = mimax(DATA['so'])\n\n\nSTD_tos = np.expand_dims(STD_tos,axis=1)\nSTD_o2 = np.expand_dims(STD_o2, axis=1)\nSTD_intpp = np.expand_dims(STD_intpp, axis=1)\nSTD_pH = np.expand_dims(STD_pH, axis=1)\nSTD_so = np.expand_dims(STD_so, axis=1)\n\n\nInput = np.append(STD_tos,STD_o2,axis=1) \nInput = np.append(Input,STD_intpp,axis=1)\nInput = np.append(Input,STD_pH,axis=1)\nInput = np.append(Input,STD_so,axis=1)\n\n\nTarget = np.expand_dims(STD_yeld, axis=1)\n\n\nInput = np.asarray(Input).astype('float32')\nTarget = np.asarray(Target).astype('float32')\n\n\nX_train, X_test= Input[0:346112,:] , Input[346112:,:]\nY_train, Y_test= Target[0:346112,:], Target[346112:,:]\nX_train = X_train.reshape((X_train.shape[0], 1,X_train.shape[1]))\nX_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n\n\ntraining_set_shape = X_train.shape\nnumber_of_channels = 1\nsample_shape = (training_set_shape[1], training_set_shape[2], number_of_channels)\nprint(f'Dataset shape: {training_set_shape}')\nprint(f'Sample shape: {sample_shape}')\n\nDataset shape: (346112, 1, 5)\nSample shape: (1, 5, 1)"
  },
  {
    "objectID": "code/modelo-PGeneral.html#modelo",
    "href": "code/modelo-PGeneral.html#modelo",
    "title": "Final Insight Report CBMC",
    "section": "MODELO",
    "text": "MODELO\n\nimport tensorflow\nfrom keras.models import Sequential\nfrom keras.layers.core import Dropout\nfrom keras.layers import  Dropout,LSTM,Dense,ReLU,Conv1D\ninput_shape = (1,5,1)\n\n\nmodel = Sequential()\nmodel.add(Dense(10,activation='softmax'))\nmodel.add(Dense(50,activation='linear'))\nmodel.add(Dense(1,activation='linear')) #softmax\nmodel.compile(loss='mse', optimizer='rmsprop' ,metrics=['poisson'])\n\n\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\n# Vijilamos el proceso de entrenamiento para detenerlo cuando encuentre un óptimo.\ncallbacks = [EarlyStopping(monitor='val_loss', patience=25), ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\n\nhistory = model.fit(X_train, Y_train, epochs=250, batch_size=1000,validation_data=(X_test, Y_test) ,verbose=1,shuffle=True, callbacks=callbacks)\n\nEpoch 1/250\n347/347 [==============================] - 2s 3ms/step - loss: 0.0053 - poisson: 0.8187 - val_loss: 0.0034 - val_poisson: 0.8155\nEpoch 2/250\n347/347 [==============================] - 1s 3ms/step - loss: 0.0037 - poisson: 0.8153 - val_loss: 0.0035 - val_poisson: 0.8156\nEpoch 3/250\n347/347 [==============================] - 1s 3ms/step - loss: 0.0037 - poisson: 0.8153 - val_loss: 0.0035 - val_poisson: 0.8156\nEpoch 4/250\n347/347 [==============================] - 1s 3ms/step - loss: 0.0037 - poisson: 0.8153 - val_loss: 0.0035 - val_poisson: 0.8156\nEpoch 5/250\n347/347 [==============================] - 1s 3ms/step - loss: 0.0036 - poisson: 0.8153 - val_loss: 0.0036 - val_poisson: 0.8157\nEpoch 6/250\n347/347 [==============================] - 1s 3ms/step - loss: 0.0036 - poisson: 0.8153 - val_loss: 0.0035 - val_poisson: 0.8157\nEpoch 7/250\n347/347 [==============================] - 1s 3ms/step - loss: 0.0036 - poisson: 0.8153 - val_loss: 0.0034 - val_poisson: 0.8155\nEpoch 8/250\n347/347 [==============================] - 1s 3ms/step - loss: 0.0036 - poisson: 0.8153 - val_loss: 0.0035 - val_poisson: 0.8156\nEpoch 9/250\n347/347 [==============================] - 1s 3ms/step - loss: 0.0036 - poisson: 0.8153 - val_loss: 0.0035 - val_poisson: 0.8156\nEpoch 10/250\n347/347 [==============================] - 1s 3ms/step - loss: 0.0036 - poisson: 0.8153 - val_loss: 0.0036 - val_poisson: 0.8157\nEpoch 11/250\n347/347 [==============================] - 1s 3ms/step - loss: 0.0036 - poisson: 0.8152 - val_loss: 0.0036 - val_poisson: 0.8157\nEpoch 12/250\n347/347 [==============================] - 1s 3ms/step - loss: 0.0036 - poisson: 0.8152 - val_loss: 0.0035 - val_poisson: 0.8156\nEpoch 13/250\n347/347 [==============================] - 1s 3ms/step - loss: 0.0036 - poisson: 0.8152 - val_loss: 0.0034 - val_poisson: 0.8155\nEpoch 14/250\n347/347 [==============================] - 1s 3ms/step - loss: 0.0036 - poisson: 0.8152 - val_loss: 0.0034 - val_poisson: 0.8155\nEpoch 15/250\n347/347 [==============================] - 1s 3ms/step - loss: 0.0036 - poisson: 0.8152 - val_loss: 0.0035 - val_poisson: 0.8156\nEpoch 16/250\n347/347 [==============================] - 1s 3ms/step - loss: 0.0036 - poisson: 0.8152 - val_loss: 0.0035 - val_poisson: 0.8156\nEpoch 17/250\n347/347 [==============================] - 1s 3ms/step - loss: 0.0036 - poisson: 0.8152 - val_loss: 0.0037 - val_poisson: 0.8158\nEpoch 18/250\n347/347 [==============================] - 1s 3ms/step - loss: 0.0036 - poisson: 0.8152 - val_loss: 0.0035 - val_poisson: 0.8156\nEpoch 19/250\n347/347 [==============================] - 1s 3ms/step - loss: 0.0036 - poisson: 0.8152 - val_loss: 0.0034 - val_poisson: 0.8156\nEpoch 20/250\n347/347 [==============================] - 1s 3ms/step - loss: 0.0036 - poisson: 0.8152 - val_loss: 0.0035 - val_poisson: 0.8156\nEpoch 21/250\n347/347 [==============================] - 1s 3ms/step - loss: 0.0036 - poisson: 0.8152 - val_loss: 0.0035 - val_poisson: 0.8156\nEpoch 22/250\n347/347 [==============================] - 1s 3ms/step - loss: 0.0036 - poisson: 0.8152 - val_loss: 0.0036 - val_poisson: 0.8157\nEpoch 23/250\n347/347 [==============================] - 1s 3ms/step - loss: 0.0036 - poisson: 0.8152 - val_loss: 0.0036 - val_poisson: 0.8157\nEpoch 24/250\n347/347 [==============================] - 1s 3ms/step - loss: 0.0036 - poisson: 0.8152 - val_loss: 0.0034 - val_poisson: 0.8155\nEpoch 25/250\n347/347 [==============================] - 1s 3ms/step - loss: 0.0036 - poisson: 0.8152 - val_loss: 0.0036 - val_poisson: 0.8158\nEpoch 26/250\n347/347 [==============================] - 1s 3ms/step - loss: 0.0036 - poisson: 0.8152 - val_loss: 0.0036 - val_poisson: 0.8157\nEpoch 27/250\n347/347 [==============================] - 1s 3ms/step - loss: 0.0036 - poisson: 0.8152 - val_loss: 0.0035 - val_poisson: 0.8156\nEpoch 28/250\n347/347 [==============================] - 1s 3ms/step - loss: 0.0036 - poisson: 0.8152 - val_loss: 0.0035 - val_poisson: 0.8156\nEpoch 29/250\n347/347 [==============================] - 1s 3ms/step - loss: 0.0036 - poisson: 0.8152 - val_loss: 0.0037 - val_poisson: 0.8158\nEpoch 30/250\n347/347 [==============================] - 1s 3ms/step - loss: 0.0036 - poisson: 0.8152 - val_loss: 0.0035 - val_poisson: 0.8156\nEpoch 31/250\n347/347 [==============================] - 1s 3ms/step - loss: 0.0036 - poisson: 0.8152 - val_loss: 0.0034 - val_poisson: 0.8155\nEpoch 32/250\n347/347 [==============================] - 1s 3ms/step - loss: 0.0036 - poisson: 0.8152 - val_loss: 0.0034 - val_poisson: 0.8155\nEpoch 33/250\n347/347 [==============================] - 1s 3ms/step - loss: 0.0036 - poisson: 0.8152 - val_loss: 0.0035 - val_poisson: 0.8156\nEpoch 34/250\n347/347 [==============================] - 1s 3ms/step - loss: 0.0036 - poisson: 0.8152 - val_loss: 0.0038 - val_poisson: 0.8160\nEpoch 35/250\n347/347 [==============================] - 1s 3ms/step - loss: 0.0036 - poisson: 0.8152 - val_loss: 0.0035 - val_poisson: 0.8156\nEpoch 36/250\n347/347 [==============================] - 1s 3ms/step - loss: 0.0036 - poisson: 0.8152 - val_loss: 0.0034 - val_poisson: 0.8155\nEpoch 37/250\n347/347 [==============================] - 1s 3ms/step - loss: 0.0036 - poisson: 0.8152 - val_loss: 0.0034 - val_poisson: 0.8155\nEpoch 38/250\n347/347 [==============================] - 1s 3ms/step - loss: 0.0036 - poisson: 0.8152 - val_loss: 0.0035 - val_poisson: 0.8157\n\n\n\nscores = model.evaluate(X_test, Y_test, verbose=1)\nprint('Test loss:', scores[0])\nprint('Test accuracy:', scores[1])\n\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n\nplt.plot(history.history['poisson'])\nplt.plot(history.history['val_poisson'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n2704/2704 [==============================] - 2s 846us/step - loss: 0.0035 - poisson: 0.8157\nTest loss: 0.003539998084306717\nTest accuracy: 0.8156507015228271\n\n\n\n\n\n\n\n\n\nte = model(X_test)\n#te = demimax(np.array(te[:,0,0]),Max_yeld,Min_yeld)\nmodel.save('Best1_model.h5')\n\n\nNew_data = DATA.loc[346112:]\nNew_data = New_data.reset_index()\n\n\nA = pd.DataFrame(np.reshape(te,(86528,)))\n\n\nA.rename(columns = {0:'Forecas_yeld'}, inplace = True)\n\n\nNew_data.insert(0,'Forecast_yeld',A)\n\n\nNew_data=New_data.set_index(['date','lat','lon'])\n\n\nDt=New_data.loc['2021-01']\nDt = Dt.reset_index()\n\n\nDt.to_csv('test_own_model.csv')"
  },
  {
    "objectID": "code/modelo-PGeneral.html#forecast-20212100",
    "href": "code/modelo-PGeneral.html#forecast-20212100",
    "title": "Final Insight Report CBMC",
    "section": "Forecast 2021:2100",
    "text": "Forecast 2021:2100\n\npre procesing the input\n\nSTD_tos = datos2['tos']\nSTD_o2 = datos2['o2']\nSTD_intpp = datos2['intpp']\nSTD_pH = datos2['pH']\nSTD_so = datos2['so']\n\nSTD_tos = np.expand_dims(STD_tos,axis=1)\nSTD_o2 = np.expand_dims(STD_o2, axis=1)\nSTD_intpp = np.expand_dims(STD_intpp, axis=1)\nSTD_pH = np.expand_dims(STD_pH, axis=1)\nSTD_so = np.expand_dims(STD_so, axis=1)\n\nInput = np.append(STD_tos,STD_o2,axis=1) \nInput = np.append(Input,STD_intpp,axis=1)\nInput = np.append(Input,STD_pH,axis=1)\nInput = np.append(Input,STD_so,axis=1)\nInput = np.asarray(Input).astype('float32')\n\nX_test= Input[:,:]\nX_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n\nPb = model.predict(X_test)\nPb =  pd.DataFrame(np.reshape(Pb,(998400,)))\nPb.rename(columns = {0:'Forecas_yeld_P'}, inplace = True)\n\n\nNew_data = datos2\nNew_data = New_data.reset_index()\nNew_data.insert(0,'Forecast_yeld_P',Pb)\nNew_data=New_data.set_index(['date','lat','lon'])\n\n\nDt=New_data.loc['2021-01':'2030-01']\n#Dt=New_data.loc['2021-01':]\nDt = Dt.reset_index()\nDt.to_csv('Forecast2_own_model.csv')\n\n\nDt=read_csv('Forecast2_own_model.csv')\n\n\nFechas=list(Dt['date'].unique())\narea=[]\nfor i in Fechas:\n    A=Dt[Dt['date']== i]\n    table = pd.pivot_table(A, values='Forecast_yeld_P', index=['lat'],columns=['lon']).fillna(0)\n    area.append(table[table > 0.44].count().sum())\n\n\narea\n\n[]\n\n\n\nFechas=list(Dt['date'].unique())\nP_mean=[]\nP_std=[]\nP_min=[]\nP_max=[]\nfor i in Fechas:\n    A=Dt[Dt['date']== i]\n    P_mean.append(A['Forecast_yeld_P'].mean())\n    P_std.append(A['Forecast_yeld_P'].std())\n    P_min.append(A['Forecast_yeld_P'].min())\n    P_max.append(A['Forecast_yeld_P'].max())\n    \nimport imageio\na=0\nfilenames = []\nli_max = Dt['Forecast_yeld_P'].max()+(Dt['Forecast_yeld_P'].max()*0.01)\nli_min = Dt['Forecast_yeld_P'].min()-(Dt['Forecast_yeld_P'].min()*0.01)\n\nfor i in Fechas:\n    A=Dt[Dt['date']== i]\n    table = pd.pivot_table(A, values='Forecast_yeld_P', index=['lat'],columns=['lon']).fillna(0)\n    fig = plt.subplots()\n\n\n    plt.subplot(3, 1, 1)\n    im = plt.imshow(table, vmin=0.410,vmax=0.485)\n    plt.colorbar(im, pad=0.01)\n    plt.suptitle(f'probabilidad de pesca por cuadrante, fecha: {i}')\n    plt.subplot(3 ,1, 2)\n    plt.plot(P_mean)\n    plt.plot(a,P_mean[a],'ro')\n    \n    plt.subplot(3 ,1, 3)\n    plt.plot(area)\n    plt.plot(a,area[a],'bo')\n    \n    a = a+1\n    \n    filename = f'A{i}.png'\n    filenames.append(filename)\n    plt.savefig(filename)\n    plt.close()\n\nwith imageio.get_writer('mygif01.gif', mode='I') as writer:\n    for filename in filenames:\n        image = imageio.imread(filename)\n        for i in range(5):\n            writer.append_data(image)\n\nfor filename in set(filenames):\n    os.remove(filename)\n\n\nprint(Max_tos,Min_tos)\nprint(Max_o2,Min_o2)\nprint(Max_intpp,Min_intpp)\nprint(Max_pH,Min_pH)\nprint(Max_so,Min_so)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Final Insight Report – McGovern Accelerator Grant",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nexecutive-summary\n\n\n \n\n\n\n\nOct 16, 2022\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nclimate-change\n\n\n \n\n\n\n\nOct 15, 2022\n\n\n15 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nfishery\n\n\n \n\n\n\n\nOct 14, 2022\n\n\n21 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nfishery\n\n\n \n\n\n\n\nOct 3, 2022\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nmethods\n\n\n \n\n\n\n\nOct 3, 2022\n\n\n4 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/an-intro-to-vms-data/index.html",
    "href": "posts/an-intro-to-vms-data/index.html",
    "title": "An Introduction to VMS Data",
    "section": "",
    "text": "Data preprocessing\nThe process of downloading and wrangling data, the datasets’ structure, and a schematic view of the analysis workflow is explained in detail in hour GitHub repository documentation: https://cbmc-gcmp.github.io/dafishr/articles/Supplementary.html through a process that if fully reproducible on a personal computer with at least 16 GB of RAM memory. We coded all this pre-processing and follow-up analysis with the R programming language (R-Core-Team 2020) using the RStudio integrated development environment (RStudio-Team 2020). For convenience we created an R package “dafishr” that is available for free download in the dedicated repository GitHub page which contains all the dependencies and functions needed.\n\n\nData cleaning and formatting\nRaw data can be downloaded by the year and are organized in several tabular files representing a monthly or by-weekly intervals. Then the preprocessing finds and eventually filter outs if needed: corrupted latitude, longitude, or speed entries that have obvious erroneous values; format dates and label corrupted ones. In the files the information stored is sometimes inconsistent and there are some errors within the rows, for example some corrupted dates or vessel codes. The most evident case came from the RLMSEP_2020/10.-OCTUBRE/16-31 OCT 2020.csv file which had all dates corrupted and we filtered it out from our analyses.\n\n\nSpatial cleaning and labeling\nAfter the cleaning step the preprocessing performs a spatial intersection with a vector file representing land to further eliminating dubious points falling inland, and after that creates a buffer of 0.025 degrees (~2.6 km) around ports to spatially label all vessel activity within port and exclude them from potential fishing activity modeling. The preprocessing then intersects all the coordinates with a polygon representing the Pacific portion of the Mexican Exclusive Economic Zone and all the Marine Protected Areas polygons (MPAs) in Mexico.\n\n\nData checks\nAfter all the cleaning steps, a global check of dates is made to be sure that all data are in the correct units, and that no vessels have wrong hours of activities assigned to them (e.g., more than 24 hours in a day). Through this step, we realized that some tracks are reported in minutes, thus we homogenized all data by hours and for those points reported in minutes we averaged the latitude and longitude coordinates.\n\n\nModeling\nFishing activity modeling\nVMS data can be modeled based on speed to infer potential fishing activities from vessel tracks and understand vessel behavior. For example, whether a vessel was cruising or if it was slowing down to deploy fishing gear. Not all the methods used to model fishing behavior are easily reproducible on a personal computer or are open source (2–6). We used a trip-based Gaussian mixture model, which has a lower maximum error rate per trip, low false-positive rates, and good performance in terms of computing efficiency when applied to VMS data (5). These characteristics allow an accurate estimation of the spatial distribution of active fishing while also being computationally efficient. We fitted the Gaussian mixture models using an expectation-maximization algorithm (6) to estimate the parameters of the multimodal speed distribution using the mixtools R package (7). We assumed three univariate normal distributions corresponding to three states of a vessel: fishing, deploying gear, and steaming. The starting values for the mean and standard deviation for each underlying distribution were estimated visually using a histogram showing the multimodal distribution of speed (Figure S2). We then defined the upper limit to the distribution for the fishing state by estimating the mean and adding two standard deviations to it. The model labels all positional records with speeds exceeding the upper limit as “not fishing” (encompassing deployment and steaming). Such labeling also grants a degree of conservatism to our model since deployment and steaming speeds sometimes overlap.\n\n\nFishing hours and catch data\nWe used only vessels that had permits for purse seine and longlines fishing gears targeting tuna, shark, and swordfish species. Once calculated the potential fishing hours from the Gaussian modeled activities, we created a timeseries of monthly total fishing hours by total number of active vessels. We then used the landings dataset to associate the same vessels by name to the catch they reported once they got back on port.\n\n\n\nWorkflow diagram representing the steps of the VMS da\n\n\n\n\n\nExample of Gaussian mixture models using an expectation-maximization algorithm (6) to estimate the parameters of the multimodal speed distribution using the mixtools R package (7). We assumed three univariate normal distributions corresponding to three states of a vessel: fishing, deploying gear, and steaming. The starting values for the mean and standard deviation for each underlying distribution are estimated visually using this histogram showing the multimodal distribution of speed.\n\n\n\n\n\nThe full footprint of the Mexican Industrial fishing fleet of the open ocean in the Mexican Pacific"
  },
  {
    "objectID": "posts/climate-change-context/index.html",
    "href": "posts/climate-change-context/index.html",
    "title": "Climate change context in the Mexican Pacific",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(patchwork)\nlibrary(fst)\nlibrary(lubridate)\nlibrary(mgcv)\nlibrary(gratia)\nThe Patrick McGovern foundation accelerator grant allowed us to explore the environmental data of the Mexican Pacific in an unprecedented way. Understanding the environmental changes to a greater resolution (meaning larger data, thus more computation needed) is critical to link it to other variables, from biological to anthropogenic. The ultimate goal of our project is to understand how industrial fishery is responding to environmental changes to predict future scenarios. In this section, we explore the environmental variability of the Mexican Pacific ( Figure 1 ), calculate its trends, and analyse the extreme events that characterized the area."
  },
  {
    "objectID": "posts/climate-change-context/index.html#sea-surface-temperature",
    "href": "posts/climate-change-context/index.html#sea-surface-temperature",
    "title": "Climate change context in the Mexican Pacific",
    "section": "Sea Surface Temperature",
    "text": "Sea Surface Temperature\nThe first key parameter to explore is Sea Surface Temperature (SST) overall trend over time. A large warming event can be seen from 2014 to 2015 ( Figure 2 ), with high average temperatures 1°C above the historical average. The temperature increase was caused by a particularly intense El Niño event which created abnormally warm waters in the Pacific. A phenomenon dubbed “the blob” caused environmental problems all over the North American coast.\n\n\nCode\nsst <- read_fst(\"../../../tropicalization_in_the_GOC/data/full_oisst_pacific_data.fst\")\n\nsst %>%\n   mutate(year = year(t)) |>\n   group_by(year) |> \n   summarise(temp = mean(temp)) |> \n   mutate(year = as.Date(paste0(year, \"-01-01\"), \"%Y-%m-%d\")) |> \n   ggplot(aes(x = year, y = temp)) +\n   geom_line(col = \"red\") +\n   geom_smooth(col = \"black\") +\n   scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n   labs(x = \"\", y = \"SST °C\") +\n   theme_minimal() +\n   theme(panel.background = element_rect(fill = NA, color = NA),\n         axis.text.x = element_text(angle = 90),\n         panel.grid.minor = element_blank(),\n         panel.grid.major.x = element_blank(),\n         text = element_text(colour = \"white\"),\n         axis.text = element_text(colour = \"white\")\n   )\n\n\n\n\n\nFigure 2: Average Sea Surface Temperature (SST) trend for the Mexican Pacific. Red line represent mean variation, black line represent the smoothed trend, and the shaded areas are the 95% confidence intervals for the mean\n\n\n\n\nOur study area is no exception, and the large warming event continued from 2014 to 2019. A decrease in average sea surface temperature occurred only in 2021 ( Figure 2 ).\n\nSST variation in space and time\nHowever, SST not only varies over time, but such variation can be of different intensities depending on the latitude. We can analyze latitude degree as a covariable along with time to understand how space influences SST variation. We can see from Figure 3 that SST has increased all over the latitudinal gradient, in particular, larger peaks can be seen in northern latitudes during the warming event.\n\n\nCode\nmod_data_lat <- sst |>\n   group_by(t, lat) |>\n   summarise(Temperature = mean(temp)) |>\n   mutate(Year = year(t), nMonth = month(t)) |>\n   group_by(Year, lat) |>\n   summarise(Temperature = mean(Temperature))\n\nfullM_sst <- bam(\n   Temperature ~\n      t2(\n         Year,\n         lat,\n         bs = c(\"tp\", \"tp\"),\n         k = c(20, 20),\n         m = 2\n      ),\n   correlation = corARMA(form = ~ 1 | Year, p = 1),\n   data = mod_data_lat,\n   method = \"REML\"\n)\n\n\nnew_data <- data.frame(lat = rep(seq(from = 15, to = 31, by = 0.1), each = 41), \n                       Year = rep(seq(from = 1982, to = 2022, by = 1), 161))\n\npred_paci <- predict(fullM_sst, new_data)\n\n(\n   year_plot <- new_data |>\n      ggplot(aes(\n         x = Year,\n         y = lat,\n         z = pred_paci,\n         fill = pred_paci\n      )) +\n      geom_raster(interpolate = TRUE) +\n      geom_contour(col = \"black\") +\n      labs(fill = \"SST °C\", y = \"Latitude\") +\n      scale_x_continuous(breaks = seq(1982, 2022, by = 2)) +\n      scale_fill_distiller(\n         palette = \"RdBu\",\n         direction = -1,\n         type = \"div\"\n      ) +\n      theme_minimal() +\n      theme(\n         panel.grid = element_blank(),\n         legend.position = \"bottom\",\n         plot.title = element_text(face = \"italic\"),\n         axis.text.x = element_text(angle = 45, hjust = 1),\n         legend.title = element_text(vjust = .8),\n         text = element_text(colour = \"white\"),\n         axis.text = element_text(colour = \"white\"),\n         panel.background = element_rect(fill = NA, color = NA)\n      )\n)\n\n\n\n\n\nFigure 3: Sea Surface Temperature (SST) variation over the latitudinal degree and time, better shows how temperature varied over time and space (south to north)\n\n\n\n\nSuch a pattern ( Figure 3 ) suggests that areas at higher latitudes can warm faster, and organisms that inhabit them can be relatively more threatened as they evolved in lower temperature regimes (Cheung, Watson, and Pauly (2013); (cheung2010?)) .\nTo model how seasonality has changed, i.e. how variation in monthly temperatures occurred from 1982 to 2022 Figure 4, we used generalized additive models described in the methods section of this site.\n\n\nCode\nsst_model <- sst %>%\n   mutate(Year = year(t),\n          nMonth = month(t),\n          fDegree = factor(round(lat, 0))) %>%\n   group_by(Year, nMonth) %>%\n   summarise(Temperature = mean(temp))\n\nknots <- list(nMonth = c(0.5, seq(1, 12, length = 10), 12.5))\n\nm_23 <- gamm(\n      Temperature ~ te(Year, nMonth, bs = c(\"cr\", \"cc\"), k = c(20, 12)),\n      data = sst_model,\n      method = \"REML\",\n      correlation = corARMA(form = ~ 1 | Year, p = 1),\n      knots = knots\n   )\n\npdat <- with(sst_model,\n             data.frame(\n                Year = rep(c(1982, 2022), each = 100),\n                nMonth = rep(seq(1, 12, length = 100), times = 2)\n             ))\n\npred <- predict(m_23$gam, newdata = pdat, se.fit = TRUE)\ncrit <- qt(0.975, df = df.residual(m_23$gam)) # ~95% interval critical t\npdat <- transform(\n      pdat,\n      fitted = pred$fit,\n      se = pred$se.fit,\n      fYear = as.factor(Year)\n   )\npdat <- transform(pdat,\n                  upper = fitted + (crit * se),\n                  lower = fitted - (crit * se))\n\np1 <- ggplot(pdat, aes(x = nMonth, y = fitted, group = fYear)) +\n   geom_line(aes(colour = fYear), size = 1.2) +    # predicted temperatures\n   labs(y = \"SST°C\",\n        x = \"Month\",\n        title = m_23$fDegree) +\n   scale_fill_viridis_d() +\n   scale_x_continuous(breaks = 1:12,\n                      labels = month.abb,\n                      minor_breaks = NULL) +\n   scale_y_continuous(\n      breaks = seq(15, 32, by = 4),\n      limits = c(15, 32),\n      minor_breaks = NULL\n   ) +\n   theme_minimal() +\n   theme(\n      panel.background = element_rect(fill = NA, color = NA),\n      axis.text = element_text(colour = \"white\"),\n      panel.grid = element_line(colour = \"gray50\"),      \n      legend.position = \"bottom\",\n      text = element_text(colour = \"white\"),\n      legend.margin = margin()\n   )\n\npred <- predict(m_23$gam, newdata = pdat, se.fit = TRUE)\ncrit <- qt(0.975, df = df.residual(m_23$gam)) # ~95% interval critical t\npdat <- transform(\n      pdat,\n      fitted = pred$fit,\n      se = pred$se.fit,\n      fYear = as.factor(Year)\n   )\npdat <- transform(pdat,\n                  upper = fitted + (crit * se),\n                  lower = fitted - (crit * se))\n\np2 <- pdat %>%\n   pivot_wider(nMonth, names_from = Year, values_from = fitted) %>%\n   mutate(diff = `2022` - `1982`) %>%\n   ggplot(aes(x = nMonth, y = diff)) +\n   geom_line(aes(col = \"°C Difference\"), size = 1.2) +\n   scale_color_manual(values = \"red\") +\n   scale_x_continuous(breaks = 1:12,\n                      labels = month.abb,\n                      minor_breaks = NULL) +\n   labs(\n      y = \"SST°C\",\n      x = \"Month\",\n      color = \"\",\n      title = m_23$fDegree\n   ) +\n   ylim(0, 1) +\n   theme_minimal() +\n   theme(\n      panel.background = element_rect(fill = NA, color = NA),\n      panel.grid = element_line(colour = \"gray50\"),\n      axis.text = element_text(colour = \"white\"),\n      legend.position = \"bottom\",\n      text = element_text(colour = \"white\"),\n      legend.margin = margin()\n   )\n\np1\np2 \n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n(b)\n\n\n\n\nFigure 4: Change of monthly temperatures caused by warming from 1982 to 2022. a) compares the seasonal trend of SST in 1982 and 2022; b) shows the difference between the two years, highlighting how temperatures have changed in magnitude\n\n\n\nOver the Mexican Pacific, summer has been 0.75 degrees Celsius higher than historical ones ( Figure 4 (b) ), and the increase in temperature that starts the summer season was anticipated by two weeks from mid-June to starting June. Conversely, the coming of the winter was delayed by two weeks. Overall, summer is one month longer in the Mexican Pacific than it used to be 30 years ago. As SST directly influences marine primary productivity, especially in oceanic waters, a more extended summer can mean a month less of marine productivity, translating to smaller secondary productivity (e.g., biomass) over the years."
  },
  {
    "objectID": "posts/climate-change-context/index.html#climate-vulnerability",
    "href": "posts/climate-change-context/index.html#climate-vulnerability",
    "title": "Climate change context in the Mexican Pacific",
    "section": "Climate vulnerability",
    "text": "Climate vulnerability\nThe best way to represent the vulnerability to climate extremes is to study extreme weather events and their frequency over time. As climate warms gradually, it is also causing an increase in frequency and magnitude of some extreme events. Recently, marine heatwaves have been defined (holbrook2020?) and we know that are causing significant effects on fisheries Cheung and Frölicher (2020) . The heatwaves analysis was calculated over each 0.1x0.1 degree pixel of the available temperature data, methods can be found here.\n\n\nCode\nMHWs_Trend <- readRDS(file = \"../../data/MHWs_nTrend.Rds\")\n\n# The base map\nmap_base <-\n   ggplot2::fortify(maps::map(fill = TRUE, plot = FALSE)) %>%\n   dplyr::rename(lon = long)\n\nggplot() +\n   geom_rect(\n      data = MHWs_Trend,\n      size = 0.2,\n      fill = NA,\n      aes(\n         x = lon,\n         y = lat,\n         xmin = lon - 0.1,\n         xmax = lon + 0.1,\n         ymin = lat - 0.1,\n         ymax = lat + 0.1\n      )\n   ) +\n   geom_raster(\n      data = MHWs_Trend,\n      aes(x = lon, y = lat, fill = slope),\n      interpolate = FALSE,\n      alpha = 0.9\n   ) +\n   scale_fill_gradient2(\n      name = \"MHWs/year (slope)\",\n      high = \"red\",\n      mid = \"white\",\n      low = \"darkblue\",\n      midpoint = 0,\n      guide = guide_colourbar(\n         direction = \"horizontal\",\n         title.position = \"top\",\n         ticks.colour = \"black\"\n      )\n   ) +\n   geom_polygon(\n      data = map_base,\n      aes(x = lon, y = lat, group = group),\n      colour = NA,\n      fill = \"grey80\"\n   ) +\n   geom_sf(\n      data = dafishr::mx_shape,\n      colour = \"black\",\n      fill = \"gray99\",\n      alpha = .2\n   ) +\n   coord_sf(xlim = c(-130, -102),\n            ylim = c(12, 31),\n            clip = \"on\") +\n   labs(x = \"\", y = \"\") +\n   theme_minimal() +\n   theme(\n      panel.background = element_rect(fill = NA, color = NA),\n      axis.text.x = element_text(angle = 90),\n      axis.text = element_text(colour = \"white\"),\n      text = element_text(colour = \"white\"),\n      legend.position = \"bottom\",\n      legend.margin = margin(),\n      legend.text = element_text(angle = 90, vjust = .5)\n   )\n\nggplot() +\n   geom_raster(data = MHWs_Trend,\n               aes(x = lon, y = lat, fill = pval),\n               interpolate = FALSE) +\n   scale_fill_manual(\n      breaks = c(\n         \"(0,0.001]\",\n         \"(0.001,0.01]\",\n         \"(0.01,0.05]\",\n         \"(0.05,0.1]\",\n         \"(0.1,0.5]\",\n         \"(0.5,1]\"\n      ),\n      values = c(\"black\", \"grey40\", \"grey60\",\n                 \"grey80\", \"grey95\", \"white\"),\n      name = \"p-value\"\n   ) +\n   geom_polygon(\n      data = map_base,\n      aes(x = lon, y = lat, group = group),\n      colour = NA,\n      fill = \"grey80\"\n   ) +\n   geom_sf(\n      data = dafishr::mx_shape,\n      colour = \"black\",\n      fill = \"gray99\",\n      alpha = .2\n   ) +\n   coord_sf(xlim = c(-130, -102),\n            ylim = c(12, 31),\n            clip = \"on\") +\n   guides(fill = guide_legend(title.position = \"top\", title.hjust = .5)) +\n   labs(x = \"\", y = \"\") +\n   theme_minimal() +\n   theme(\n      panel.background = element_rect(fill = NA, color = NA),\n      axis.text.x = element_text(angle = 90),\n      axis.text = element_text(colour = \"white\"),\n      legend.position = \"bottom\",\n      text = element_text(colour = \"white\"),\n      legend.margin = margin(),\n      legend.title = element_text(face = \"italic\")\n   )\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n(b)\n\n\n\n\nFigure 5: Map of heatwaves frequency in the Mexican Pacific: a) represents the slope increase or decrease; a) represents the significance of the slope\n\n\n\nAs extreme Marine Heatwaves (MHWs) increased all over the Mexican Pacific Figure 5 (a) the California Current directly influences an area that shows an inverse trend. A negative trend in MHWs suggests that this area might be critical as a refuge area and hotspot of future productivity under climate change scenarios. Identifying these regional differences underlines the importance of these smaller-scale analyses to understand the magnitude of localized climate impacts better. In the next sections, we will compare this spatio-temporal variation to fishing activity to understand its response."
  },
  {
    "objectID": "posts/executive-summary/index.html",
    "href": "posts/executive-summary/index.html",
    "title": "Executive summary:  the McGovern accelerator grant",
    "section": "",
    "text": "We were able to reinvent our data pipeline using Amazon Web Services, enhance our computational capabilities, and test Machine Learning (ML) methods applied in a novel way to fisheries. We present our insights on this webpage, which is intended as a dynamic way to showcase results. We will maintain and update this page along with the results of our analysis in the future. The structure of this website is the following:\n\nClimate change context in the Mexican Pacific\nIndustrial fishery in the Mexican Pacific under Climate Change\nAn introduction to VMS data\nMaterials and methods\n\nAs a result of our analyses, we advanced our understanding of fishery and climate change, and these were the major insights obtained:\n\nSea Surface Temperature changed significantly over time, and strong MHWs events occurred more frequently in the area.\nThe Southern California Current is creating a buffer in front of the Baja California Peninsula, which could represent a key refuge area for species and the last standing pillar of marine productivity in the Mexican Pacific.\nSummer in the Mexican Pacific arrives two weeks earlier and ends two weeks later than the historical average.\nThe catches of the fishing industry operating in the area were significantly affected, with a reduction of 22% [95%CI: -50%, -6%] in average Catch per Unit of Effort (CPUE) during the MHWs event and an overall decreasing trend of 80.7 total CPUE each year in total.\nThe fishing industry is spatially highly adaptable and is shifting towards high seas and more profitable waters to maintain catches.\nOur Machine Learning model predicts that a decrease of 0.55 CPUE per year up to 2030.\n\nThis data will be critical in monitoring the effects of climate change on the fishing industry; however, large computational capabilities are essential to generate space and time-explicit models to depict a complete picture of the complexity of climate change and industrial fishery. The industrial fishery is modern and highly adaptable, so it can counteract climate change effects by changing fishing areas. However, such displacement has ecological consequences. It might be detrimental to fishery sustainability in the long term as the maintenance of the catch, and the absence of a collapse might be caused to an increase in space use, not to the resilience of the natural system. Fishery management must consider this for a policy if sustainable fisheries are a genuine objective.\nOur analysis also underlines the importance of geospatial data like VMS for fishery assessments. We hope more researchers will use our package to download and analyze data to provide further insights.\nAll data needed to replicate the analysis showcased here can be downloaded from here. Full datasets of permits, landings, and VMS data is available through our R package dafishr."
  },
  {
    "objectID": "posts/fishery-context/index.html",
    "href": "posts/fishery-context/index.html",
    "title": "Industrial fishery in the Mexican Pacific under Climate Change",
    "section": "",
    "text": "Code\n# Loading libraries -------------------------------------------------------\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(mgcv)\nlibrary(gratia)\nlibrary(doParallel) \n\nvms <- read_rds(\"../../data/vms_spatial.RDS\")\nlandings <- read_rds(\"../../data/landings.RDS\")\nNow that we know what happened to the Mexican Pacific regarding climate change, we show the dynamics we know of the Mexican fishing industry, specifically the pelagic fleet, which is the target of our project. The Mexican pelagic fleet targets tuna, sharks, swordfish, and other pelagic animals using purse seiners and long lines. The fleet comprises around 100 powerful high-seas vessels that can store from 500 to 1,200 tons of catch in one trip.\nA first exploration to understand their traits is to see if their landings changed over time ( Figure 2 ). In fishery science usually catches are expressed in terms of Catches per Unit of Effort (CPUE) which standardize the catches for the amount of time needed to collect it. We can see a trend of CPUE in Figure 2 .\nIt is reasonable to expect a decrease in catches, especially after seeing the effect temperature had over the Pacific. Average CPUE shows a significant (p < 0.01) decrease of 22% [95%CI: -50%, -6%] during the warming period ( Figure 2 (a) ), whereas if we look at total CPUE, an overall decreasing trend of 80.7 total CPUE (in metric tons) lost each year ( Figure 2 (b) ). It seems that warming events are negatively influencing the catches. Particularly, summer months are the ones with larger impacts, with overall decreases in CPUE from May to October (Figure 3 (b))."
  },
  {
    "objectID": "posts/fishery-context/index.html#space-time-model-of-fishing-yields",
    "href": "posts/fishery-context/index.html#space-time-model-of-fishing-yields",
    "title": "Industrial fishery in the Mexican Pacific under Climate Change",
    "section": "Space-time model of fishing yields",
    "text": "Space-time model of fishing yields\nThe industrial fishing fleet is highly adaptable. When fishing is discussed, one often pictures an artisanal fisherman roaming coastal waters. However, the reality is that fisheries are highly industrialized, and Mexico is no exception. Some of these vessels’ capacity is large regarding autonomy at sea and catch yield ( Figure 1 ).\nSuch a capacity allows them to counteract climate change events by changing fishing grounds, or increasing fishing efforts. The most important fishing grounds are located northward, and fishing activity has increased at higher latitudes (32 degrees). Such a shift is related to species redistribution as noted by previous studies Cheung et al. (2010) .\nVessels also used a specific part of the Mexican Pacific from -100 to -120 degrees of latitude. The area corresponds to the influence of the Southern California Current. It underlines the importance of the functioning of this current for marine productivity and the fishery it sustains.\n\n\nCode\nmod_data_lat <- vms |>\n      group_by(date, latitude) |>\n      summarise(relative = mean(relative)) |>\n      mutate(Year = year(date), nMonth = month(date)) \n\nfullM_sst <- bam(\n      relative ~\n            t2(\n                  Year,\n                  latitude,\n                  bs = c(\"tp\", \"tp\"),\n                  k = c(10, 10),\n                  m = 2\n            ),\n      correlation = corARMA(form = ~ 1 | Year, p = 1),\n      data = mod_data_lat,\n      method = \"REML\"\n)\n\nnew_data <-\n      data.frame(latitude = rep(seq(\n            from = -10, to = 31, by = 0.1\n      ), each = 13),\n      Year = rep(seq(\n            from = 2009, to = 2021, by = 1\n      ), 411))\n\npred_paci <- predict(fullM_sst, new_data)\n\nlat_plot <- new_data |>\n         ggplot(aes(\n               x = Year,\n               y = latitude,\n               z = pred_paci,\n               fill = pred_paci\n         )) +\n      geom_raster(interpolate = TRUE) +\n      geom_contour(col = \"black\") +\n      labs(fill = \"Relative CPUE\", y = \"Latitude\") +\n      scale_x_continuous(breaks = seq(2008, 2022, by = 2)) +\n      scale_y_continuous(breaks = seq(-10, 32, by = 2)) +\n      scale_fill_distiller(\n            palette = \"RdBu\",\n            direction = -1,\n            type = \"div\"\n      ) +\n      theme_minimal() +\n      theme(\n            panel.grid = element_blank(),\n            legend.position = \"bottom\",\n            plot.title = element_text(face = \"italic\"),\n            axis.text.x = element_text(angle = 45, hjust = 1),\n            legend.title = element_text(vjust = .8),\n            text = element_text(colour = \"white\"),\n            axis.text = element_text(colour = \"white\"),\n            panel.background = element_rect(fill = NA, color = NA)\n      )\n\nmod_data_lat_long <- vms |>\n      mutate(Year = year(date), nMonth = month(date)) |> \n      group_by(Year, nMonth, longitude) |> \n      summarise(relative = mean(relative))\n       \n\nfullM_sst <- bam(\n      relative ~\n            t2(\n                  Year,\n                  longitude,\n                  bs = c(\"tp\", \"tp\"),\n                  k = c(10, 10),\n                  m = 2\n            ),\n      correlation = corARMA(form = ~ 1 | Year, p = 1),\n      data = mod_data_lat_long,\n      method = \"REML\"\n)\n\nnew_data_long <-\n      data.frame(longitude = rep(seq(\n            from = -140.0, to = -87.0, by = 0.1\n      ), each = 13),\n      Year = rep(seq(\n            from = 2009, to = 2021, by = 1\n      ), 531))\n\npred_paci_long <- predict(fullM_sst, new_data_long)\n\nlong_plot <- new_data_long |>\n      ggplot(aes(\n            x = Year,\n            y = longitude,\n            z = pred_paci_long,\n            fill = pred_paci_long\n      )) +\n      geom_raster(interpolate = TRUE) +\n      geom_contour(col = \"black\") +\n      labs(fill = \"Relative CPUE\", y = \"Longitude\") +\n      scale_x_continuous(breaks = seq(2008, 2022, by = 2)) +\n      scale_y_continuous(breaks = seq(-140, -87, by = 2)) +\n      scale_fill_distiller(\n            palette = \"RdBu\",\n            direction = -1,\n            type = \"div\"\n      ) +\n      theme_minimal() +\n      theme(\n            panel.grid = element_blank(),\n            legend.position = \"bottom\",\n            plot.title = element_text(face = \"italic\"),\n            axis.text.x = element_text(angle = 45, hjust = 1),\n            legend.title = element_text(vjust = .8),\n            text = element_text(colour = \"white\"),\n            axis.text = element_text(colour = \"white\"),\n            panel.background = element_rect(fill = NA, color = NA)\n      )\nlat_plot\nlong_plot\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n(b)\n\n\n\n\nFigure 4: Spatiotemporal variation in relative catch per unit of effort (CPUE) in the Mexican Pacific by a) Latitude, and b) longitude\n\n\n\nIndustrial vessels also are stretching further at sea, pushed by the overexploitation of coastal areas and the improvement of marine technologies. Exploiting the high seas ( Figure 5 ) represents an ecological threat, and fishery management becomes more complex to implement.\nThis spatial displacement allows industrial fisheries to maintain their catches in an overexploited Mexican Pacific. Therefore, if we look at just the catches, we would not be able to understand the complete picture that the spatial displacement is showing. This underlines the importance of the accelerator grant that aided us in deploying sufficient computational power to analyze the entire Vessel Monitoring System database (see VMS section for further description of the dataset).\n\n\nCode\nvms_Trend <- readRDS(file = \"../../data/vms_Trend.RDS\")\n\nmap_base <-\n      ggplot2::fortify(maps::map(fill = TRUE, plot = FALSE)) %>%\n      dplyr::rename(lon = long)\n\nvms_tomap <- vms_Trend |>\n      mutate(lat = round(lat), lon = round(lon)) |>\n      group_by(lon, lat) |>\n      summarise(slope = mean(slope)) |>\n      mutate(slope = round(slope, 2)) |>\n      filter(slope != 0)\n\nmap_slope <- ggplot() +\n      geom_raster(\n            data = vms_tomap,\n            aes(\n                  x = lon,\n                  y = lat,\n                  fill = log1p(slope),\n                  col = \"black\"\n            ),\n            interpolate = FALSE,\n            alpha = 0.9\n      ) +\n      scale_fill_gradient2(\n            name = \"Relative CPUE/year (slope)\",\n            high = \"firebrick\",\n            mid = \"gray90\",\n            low = \"darkblue\",\n            na.value = NA,\n            midpoint = 0,\n            guide = guide_colourbar(\n                  direction = \"horizontal\",\n                  title.position = \"top\",\n                  ticks.colour = \"black\"\n            )\n      ) +\n      geom_polygon(\n            data = map_base,\n            aes(x = lon, y = lat, group = group),\n            col = NA,\n            fill = \"grey80\"\n      ) +\n      geom_sf(\n            data = dafishr::mx_shape,\n            col = \"black\",\n            fill = \"gray99\",\n            alpha = .2\n      ) +\n      coord_sf(xlim = c(-150, -80),\n               ylim = c(-10, 32),\n               clip = \"on\") +\n      labs(x = \"\", y = \"\") +\n      theme_minimal() +\n      theme(\n            panel.background = element_rect(fill = NA, color = NA),\n            plot.background = element_rect(fill = NA, color = NA),\n            axis.text.x = element_text(angle = 90),\n            legend.position = \"bottom\",\n            legend.text = element_text(angle = 90, vjust = .5),\n            text = element_text(color = \"white\"),\n            axis.text = element_text(color = \"white\")\n      )\n\nmap_slope \n\n\n\n\n\nFigure 5: Spatiotemporal variation in relative catch per unit of effort (CPUE) in the Mexican Pacific by a) Latitude, and b) longitude\n\n\n\n\n\nMachine Learning forecast\n\n\nCode\nml_output <- read_csv(\"../../data/forecast_own_model.csv\")\nml_output |> \n      ggplot(aes(x = year, y = final_yeld)) +\n      geom_point(col = \"red\") +\n      geom_smooth(method = \"lm\", col = \"red\") +\n      labs(x = \"Years\", y = \"CPUE\") +\n      theme_minimal() +\n      theme(\n            panel.background = element_rect(fill = NA, color = NA), \n            panel.grid = element_line(color = \"gray60\"), \n            plot.background = element_rect(fill = NA, color = NA),\n            axis.text.x = element_text(angle = 90),\n            legend.position = \"bottom\",\n            legend.text = element_text(angle = 90, vjust = .5),\n            text = element_text(color = \"white\"),\n            axis.text = element_text(color = \"white\")\n      )\n\n\n\n\n\nFigure 6: Prediction of average CPUE due to climate change effects\n\n\n\n\nThe machine learning model uses temperature, oxygen concentration, pH, and marine primary productivity to forecast overall CPUE for the industrial fishing fleet up to 2030 (see methods for further details) . We found a significant decrease of CPUE (slope = -0.55, 95% CI [-0.72, -0.37], p < .001) over time Figure 6, confirming the effects of temperature variation on the potential fishing areas in the Mexican Pacific. The model therefore forecasts that by 2030 the average CPUE for the industrial vessels will be around 15 (CPUE, in tons) instead of the average of 28 of 2008-2015 period."
  },
  {
    "objectID": "posts/methods/index.html",
    "href": "posts/methods/index.html",
    "title": "Materials and methods",
    "section": "",
    "text": "Marine Heatwaves events\nTo detect marine heatwaves events, we calculated our climatological period (the statistical properties of the time series, including the mean, variance, seasonal cycle, and quantiles) over the whole time series. We then detected marine heatwaves from SST daily data within each 0.25-degree grid (i.e., each pixel). Since the analysis is pixel-based, it is independent of the size of the region bounding box selected, which is reported in Figure S1. The number and duration of marine heatwaves were calculated as periods of five or more consecutive days when daily SST was greater than the 90th percentile of our climatological threshold based on our time series ( Hobday et al. (2016) ). After detecting the events, we fit a generalized linear model (GLM) with a Poisson distribution to each pixel to calculate rates of change in marine heatwaves frequency. The marine heatwaves analysis was completed using the R package heatwaveR ( Schlegel and Smit (2018) ).\n\n\nTemperature space-time models\nAfter the larger scale analyses, we modeled temperature changes over space and time. To achieve this, we modeled the SST data using Generalized Additive Models (GAMs). To allow for interaction between Year and Latitude, we fit the following model: \\(y = β0 + f(𝑥1, 𝑥2) + 𝜀, 𝜀 ~N(0, 𝜎2Λ)\\)\nwhere β0 is the intercept and f(𝑥1, 𝑥2) represents the smooth interaction of Year and Latitude. We used a tensor product for the smooth interaction which is especially useful for representing functions of covariates measured in different units. This spline interaction also has another advantage: in a classic time plus space model the terms are purely additives, thus no matter the degree of latitude we are predicting, the latitudinal effect for that particular degree will always be the same for all the time series. Whereas, if we use this type of spline interaction model, we can relax this additivity assumption and fit a model that allows the latitudinal part of the model to change in time along with the trend. Finally, to account for autocorrelation, we used an ARMA correlation structure. The best autocorrelation structure was chosen based on the AIC value and the results of a generalized likelihood ratio test ( Simon N. Wood (2017) ).\nWe used a similar approach for our analysis of the seasonal effects (Month) to know how much the temperature changed on a month-to-month basis over the years. In this analysis, we wanted to model any variation in, or interaction between, the trend and seasonal features of the data. We used the same formula (1) as for the space-time model but this time we modeled Year and Month as covariates. For the Month term, we used a cyclic cubic spline, as there should be no discontinuity between January and December. However, since we will be using monthly averaged data, to avoid jumps in SST values that can be quite different (e.g., December 1st vs January 1st) we created smooth boundary knots such that the distance between December and January is the same as the distance between any other month. In practice, we achieved this by using knots at 0.5 and 12.5 and creating a sequence spreading evenly between 1 and 12.\nAs a statistical protocol for our GAM analysis, we followed commonly applied methods to check the validity of the model fit as well as autocorrelation problems ( Zuur and Ieno (2016); Simon N. Wood (2017) ). We used the mgcv R package ( S. N. Wood (2011) ) for all the GAM analyses in this project.  \n\n\nMachine Learning model\nWe used a deep learning approach to forecast the future spatial suitability of fishing activity according to climate change scenarios.\nData used to predict spatial suitability was Sea Surface Temperature, Dissolved Oxygen Concentration, pH, and Primary Organic Production by all types of phytoplankton. All data are monthly records from 1850 to 2100 over a 0.5-degree pixel resolution. We used only data from 2008 to match VMS data timescale.\nData within each pixel are standardized and divided into 80% training and 20% test datasets.\nThe model is an artificial neural network with an activation function softmax and an entry shape of (1,5,1) dimensions. There is also a hidden layer of 50 neurons, a linearly activated function wholly connected. We apply a dropout filter of 0.2. The exit layer is of one neuron and a linear function.\nThe model was compiled using the MSE as an optimization index, an optimization algorithm of RMSPROP, and a Poisson metric optimization.\n\nML model parameters. Total parameters: 661; trainable parameters: 661; non-trainable parameters: 0.\n\n\nLayer (type)\nOutput Shape\nParam #\n\n\n\n\ndense_15\nNone, 1, 10\n60\n\n\ndense_16\nNone, 1, 50\n550\n\n\ndense_17\nNone, 1, 1\n51\n\n\n\nFor the training parameters, we used epochs = 250 and batch = 1000.\nThe model accuracy scored 0.81.\n\n\n\n\n\nThe training stopped after 25 iterations when a local solution with an optimization value of MSE = 0.00354 and a Poisson loss = 0.815 was found.\nThe code to reproduce the ML model is available here, which was run on a SageMaker studio environment.\n\n\n\n\n\nReferences\n\nHobday, Alistair J., Lisa V. Alexander, Sarah E. Perkins, Dan A. Smale, Sandra C. Straub, Eric C. J. Oliver, Jessica A. Benthuysen, et al. 2016. “A Hierarchical Approach to Defining Marine Heatwaves.” Progress in Oceanography 141: 227238. https://doi.org/10.1016/j.pocean.2015.12.014.\n\n\nSchlegel, Robert W., and Albertus J. Smit. 2018. “A Central Algorithm for the Detection of Heatwaves and Cold-Spells.” Journal of Open Source Software 27 (3): 821.\n\n\nWood, S. N. 2011. “Fast Stable Restricted Maximum Likelihood and Marginal Likelihood Estimation of Semiparametric Generalized Linear Models.” Journal of the Royal Statistical Society (B) 73 (1): 336.\n\n\nWood, Simon N. 2017. Generalized Additive Models. https://doi.org/10.1201/9781315370279-5.\n\n\nZuur, Alain F., and Elena N. Ieno. 2016. “A Protocol for Conducting and Presenting Results of Regression-Type Analyses.” Methods in Ecology and Evolution 7 (6): 636645. https://doi.org/10.1111/2041-210x.12577."
  }
]